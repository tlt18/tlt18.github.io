---
title: An overview of Robust RL
tags: RL
key: 2023-07-05
author: tlt22
modify_date: 2023-07-05
# article_header:
#   type: cover
#   image:
#     src: /fig/2021-03-28/image-20210329082727723.png
---

This post introduces robust reinforcement learning algorithms against transition model uncertainty.

<!--more-->

## Indirect RL: robust Bellman equation

If $\mathcal{U}=\otimes_{(s,a)}\mathcal{U}_{(s,a)}$,

$$
\begin{align}
v^\pi_\mathcal{U}(s)
&=\sum_{a_0}\pi(a_0|s_0)(R(s_0,a_0)+\gamma\min_{P(\cdot|s_0,a_0)\in\mathcal{U}_{(s_0,a_0)}}\sum_{s^\prime}P(s^\prime|s_0,a_0)
v_{\mathcal{U}}^\pi(s^\prime))
\end{align}
$$

If $\mathcal{U}=\otimes_{s}\mathcal{U}_{s}$,

$$
\begin{align}
v^\pi_\mathcal{U}(s)
&=\min_{(P(\cdot|s_0,\cdot),R(s_0,\cdot))\in\mathcal{U}_{s_0}}
\sum_{a_0}\pi(a_0|s_0)(R(s_0,a_0)+\gamma\sum_{s^\prime}P(s^\prime|s_0,a_0)
\min_{(P,R)\in\mathcal{U}/\mathcal{U}_{s_0}}v_{(P,R)}^\pi(s^\prime))\\
&=\min_{(P(\cdot|s_0,\cdot),R(s_0,\cdot))\in\mathcal{U}_{s_0}}
\sum_{a_0}\pi(a_0|s_0)(R(s_0,a_0)+\gamma\sum_{s^\prime}P(s^\prime|s_0,a_0)
\min_{(P,R)\in\mathcal{U}}v_{(P,R)}^\pi(s^\prime))\\
&=\min_{(P(\cdot|s_0,\cdot),R(s_0,\cdot))\in\mathcal{U}_{s_0}}
\sum_{a_0}\pi(a_0|s_0)(R(s_0,a_0)+\gamma\sum_{s^\prime}P(s^\prime|s_0,a_0)
v_{\mathcal{U}}^\pi(s^\prime))\\
\end{align}
$$

- **Faster Last-iterate Convergence of Policy Optimization in Zero-Sum Markov Games**: PIM borrows from ordinary matrix game updating methods, regularized PIM
- **A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games**: infinity PEV + H-step PIM
- **On the Stability and Convergence of Robust Adversarial Reinforcement Learning: A Case Study on Linear Quadratic Systems**: Convergence of adversarial RL in LQ case


## Direct RL: robust policy grandient

- **On the Convergence of Policy Gradient in Robust MDPs**: convergence of robust PG, update pi using projection descent in outer loop (gradient domination condition)
- **Policy Gradient Method For Robust Reinforcement Learning**: convergence proof of robust PG with R-contamination set using log-sum-exp approximation
- **Policy Gradient for s-Rectangular Robust Markov Decision Processes**: robust PG with Lp norm uncertainty set
- **Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games**: convergence of robust PG with entropy regularization

## Methods for minimization in term of transition probability

### Constraint opt. in dynamics space

#### Close-form/Regularization

- **Online Robust Reinforcement Learning with Model Uncertainty**: robust PEV with R-contamination & uncertainty set estimation
- **Policy Gradient for s-Rectangular Robust Markov Decision Processes**: robust PG with Lp norm uncertainty set
- **Efficient Policy Iteration for Robust Markov Decision Processes via Regularization**: s-/(s,a)-rectangular Lp robust MDP self-consistent and Bellman operator to accelerate solving RMDP
- **Maximum Entropy RL (Provably) Solves Some Robust RL Problems**: MaxEnt RL is robust to some degree of misspecification in the reward function

#### Language duality

- **Robust Reinforcement Learning using Offline Data**(Robust FQI): opt. by language duality & conjugate function on offline setting

#### QP

- **Wasserstein Robust Reinforcement Learning**(WR2L): QP by zero-order gradient estimate with Wasserstein set constraint

#### Other constraint opt.

- **Robust Phi-Divergence MDPs**: simplex projections opt. with $\phi$-divergence set

### Convert to other space

#### Adversary action

- **Robust Adversarial Reinforcement Learning**(RARL): adversary action

#### Dynamics parameter

- **Robust Reinforcement Learning for Continuous Control with Model Misspecification**(RE-MPO): MPO + opt. in finite uncertainty set

#### State space

- **Learning Robust Policy against Disturbance in Transition Dynamics via State-Conservative Policy Optimization**: Wasserstein uncertainty set to $\epsilon$-ball in state space with Â strong duality assumption + gradient descent in state space
- **Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees**: state constraint with adversary state NN