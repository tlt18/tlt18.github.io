---
title: 强化学习：蒙特卡洛方法
tags: RL
key: 2021-03-28
author: tlt18
modify_date: 2021-03-30
---

蒙特卡洛方法(Monte Carlo method)是通过采样，用某个随机事件出现的频率估计该随机事件的概率、随机变量的期望的方法。本文将介绍在强化学习中利用蒙特卡洛方法估计动作价值$q_\pi(s,a)$(预测问题)、更新策略(控制问题)，并且更加关注其中的数学原理。

<!--more-->

---

## 1 预测问题 Monte Carlo Prediction

### 1.1 基本方法

预测问题的目标是，在给定一个策略$\pi$后，计算动作价值函数$q_\pi(s,a)$。根据定义，动作价值函数是在给定的状态s、动作a下，累计回报G的期望：

$$
q_\pi(s,a)=\mathbb{E_\pi}[G_t \mid S_t=s,A_t=a]
$$

其中累计回报G是从当前时刻开始，折现回报之和：

$$
G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
$$

使用蒙特卡洛方法，是将动作价值计算公式中，累计回报G的数学期望，替换成采样累计回报G的平均值：

$$
q_{\pi}(s,a) \approx \frac{1}{n} \sum_{i=1}^{n} G_{i}
$$

其中采样时采用策略$\pi$，生成一系列观测片段，就能保证是在$\pi$产生的分布下计算的期望。例如我们得到一个观测片段$
S_{1}, A_{1}, R_{2}, \ldots, S_{t}=s, A_{t}=a, R_{t+1}, \ldots, S_{T}, A_{T}, R_{T+1}
$,得到累计回报G的一个采样：

$$
G_{i}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-t} R_{T+1}
$$


多次采样后，得到针对(s,a)的一系列累计回报$G_1,G_2,\cdots,G_n$，取均值就得到$q_{\pi}(s,a)$的一个估计。

![image-20210329081756148](https://github.com/tlt18/tlt18.github.io/raw/master/fig/2021-03-28/image-20210329081756148.png)

### 1.2 首次访问MC和每次访问MC

首次访问和每次访问是针对一个episode而言的。在一个episode中，同一个(s,a)可能多次被访问到，其中第一次访问(s,a)称为首次访问，因而有如下区别：

- 首次访问MC算法：用**首次访问**的累计回报G估计动作价值$q_\pi(s,a)$；
- 每次访问MC算法：用**所有访问**的累计回报G估计动作价值$q_\pi(s,a)$。

### 1.3 回溯图backup diagram

![image-20210329082727723](https://github.com/tlt18/tlt18.github.io/raw/master/fig/2021-03-28/image-20210329082727723.png)

### 1.4 策略评估的收敛性

根据大数定量，需要有无穷多个episode的样本序列才能使动作价值收敛，算法实现上可以通过两种方法去除这个假设：

- 评估动作价值$q_\pi(s,a)$时分析评估值和收敛值的误差和出现概率的上下界，通过足够多的episode保证这些界足够小；
- 直接不再要求策略改进前就直接完成策略评估，不管收不收敛，最极端的例子就是DP中的价值迭代。

---

## 2 控制问题 Monte Carlo Control

### 2.1 基本方法

控制问题的目标是求解最优策略。基本思想是广义策略迭代(GPI)：当前价值函数不断迭代近似当前策略的真实价值函数(策略评估)，当前策略根据当前策略不断调优(策略改进)。



![image-20210329083416841](https://github.com/tlt18/tlt18.github.io/raw/master/fig/2021-03-28/image-20210329083416841.png)



### 2.2 策略改进

#### 改进方法

对于每个状态s，策略改进的贪心策略为选择动作价值函数最大的动作：



$$
\pi(s) = \mathop{\arg\max}\limits_{a}q(s,a)
$$

下面是蒙特卡洛控制的完成算法，其中的Exploring Starts将会在下面介绍。

![image-20210330083123879](https://github.com/tlt18/tlt18.github.io/raw/master/fig/2021-03-28/image-20210330083123879.png)

#### 改进的证明

下面说明这样的改进能使策略变优。假设$q_{\pi _k}(s,a)$是策略$\pi _k$评估出的动作价值，那么改进后的策略$\pi _{k+1}$为$q_{\pi _k}(s,a)$对应的贪心策略，对任意的状态s：



$$
\begin{aligned}
q_{\pi_{k}}\left(s, \pi_{k+1}(s)\right) &=q_{\pi_{k}}\left(s, \underset{a}{\arg \max } q_{\pi_{k}}(s, a)\right) \\
&=\max _{a} q_{\pi_{k}}(s, a) \\
& \geq q_{\pi_{k}}\left(s, \pi_{k}(s)\right) \\
& \geq v_{\pi_{k}}(s)
\end{aligned}
$$



- 第一、二个等号是根据贪心策略的定义；
- 第三个不等号是根据最大值的放缩；
- 最后一个不等号我不理解成立的原因，等以后理解了再填上这个坑。我可以理解的是：


$$
\underset{a}{\max } q_{\pi_{k}}(s, a) \geq \underset{a} \sum \pi{_k}(a \mid s) q_\pi{_k}(s,a) = v_{\pi_{k}}(s)
$$



得到不等式$q_{\pi_{k}}\left(s, \pi_{k+1}(s)\right) \geq v_{\pi_{k}}(s)$后，根据策略改进定理可以证明(这个坑也以后再填，参考Sutton book4.2的证明)：


$$
v_{\pi_{k+1}}(s) \geq v_{\pi_{k}}(s)
$$


这样就能保证策略$\pi_{k+1}$比$\pi_{k}$更优，除非已经收敛到最优策略了。

---

## 3 MC方法的探索性 Exploration

Exploration是为了让智能体获取更多的环境信息，Exploitation是为了利用已知环境信息进行策略评估和改进。如果仅靠上面提到的策略迭代方法不能保证算法收敛，因为一旦找到一个较好的(s,a)，往往会已知沿着这个策略进行下去，而陷入局部解，对于当前看来次优的(s,a)就没有进行充分的评估(甚至还没有访问)。因此想要保证每一个(s,a)都能被访问到，使算法收敛到最优解，需要保证MC方法的探索性。

### 3.1 试探性出发Exploring Starts

指定一个(s,a)作为episode采样的开始，每一个(s,a)二元组都能被非0的概率被选为起点。

### 3.2 同轨策略on-policy

#### 同轨策略和离轨策略

如果想避免采用试探性出发，可以有两种方法保证MC在选择动作时保持exploration，即同轨策略On-policy和离轨策略Off-policy。

- on-policy：生成采样数据序列的策略(动作策略behavior policy)和用于实际决策的待评估和改进的策略(目标策略target policy)相同，其中试探性出发就是on-policy的一个例子。
- off-policy：动作策略behavior policy和目标策略target policy不同。

#### 基本方法

在on-policy策略方法中，策略往往是soft的，也就是

$$
\pi(s \mid a)>0,\forall s \in \mathbb{S}, \forall a \in \mathbb{A}
$$



而$\varepsilon-soft$策略的意思是


$$
\pi(s \mid a)>\frac{\epsilon}{\mid \mathbb{A(s)} \mid},\forall s \in \mathbb{S}, \forall a \in \mathbb{A}
$$



其中$\varepsilon-soft$的一个特例就是$\varepsilon-greedy$，在绝大多时候选择最大估计动作价值所对应的动作，并以$\varepsilon$的概率随机选择一个动作。


$$
\pi(a \mid s)=\left\{\begin{array}{ll}
1-\varepsilon+\frac{\varepsilon}{\mathbb{A(s)}} & \text { if } a=\underset{a \in \mathbb{A}}{\arg \max } Q(s, a) \\
\frac{\varepsilon}{\mathbb{A(s)}} & \text { otherwise }
\end{array}\right.
$$

下面是On-policy首次访问MC控制的算法框图：

![image-20210330090619379](https://github.com/tlt18/tlt18.github.io/raw/master/fig/2021-03-28/image-20210330090619379.png)

#### 改进的证明

下面证明，对于任意的一个$\varepsilon-soft$的策略$\pi$，根据$q_\pi(s,a)$生成的$\varepsilon-greedy$策略$\pi'$是对$\pi$的改进，对$\forall s \in \mathbb{S}$：
$$
\begin{aligned}
q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a} \pi^{\prime}(a \mid s) q_{\pi}(s, a) \\
&=\frac{\epsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \max _{a} q_{\pi}(s, a) \\
& \geq \frac{\epsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \sum_{a} \frac{\pi(a \mid s)-\frac{\epsilon}{|\mathcal{A}(s)|}}{1-\varepsilon} q_{\pi}(s, a) \\
&=\frac{\epsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)-\frac{\epsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+\sum_{a} \pi(a \mid s) q_{\pi}(s, a) \\
&=v_{\pi}(s)
\end{aligned}
$$

- 第一个等号是$q_{\pi}(s,\pi')$的定义，之前动作价值的第二个自变量都是动作a，这里换成一个策略，就是按照策略对每个动作的价值求期望；
- 第二个等号是根据$\pi'$的$\varepsilon-greedy$性质展开；
- 第三个不等号是对a取最大值放缩为不同a的加权组合，其中每个权重在0到1之间，并且权重和为1。更常见的加权方法是用权重$\pi(a \mid s)$对每个动作a加权，比较这两种加权方法的区别：

$$
\pi(a \mid s)>\frac{\pi(a \mid s)-\frac{\varepsilon}{\mid \mathbb{A} \mid}}{1 - \varepsilon}\
\iff \frac{1}{\mid \mathbb{A} \mid} >\pi(a \mid s)
$$

​      当$\pi$采取某个动作a的概率$\pi(a \mid s)$小于随机概率$\frac{1}{\mid \mathbb{A} \mid}$时，权重$\frac{\pi(a \mid s)-\frac{\varepsilon}{\mid \mathbb{A} \mid}}{1 - \varepsilon}$会被抑制，即小于$\pi(a \mid s)$；反之某个动作a的概率大于随机概率，权重会被加强。特别地，当策略$\pi$是$\varepsilon-greedy$时，贪心动作的权重为1，其余动作的权重为0。这里进行权重改进的前提是，$\pi$是$\varepsilon-soft$的；

- 第四、五个等号是放缩的结果进行整理。

在得到$q_\pi(s,\pi'(s))>v_\pi(s)$后，根据策略改进定理可以证明，对$\forall s \in \mathbb{S}$

$$
v_\pi'(s) \geq v_\pi(s)
$$
从而我们证明了，对于任意的一个$\varepsilon-soft$的策略$\pi$，根据$q_\pi(s,a)$生成的$\varepsilon-greedy$策略$\pi'$是对$\pi$的改进。

### 3.3 离轨策略off-policy

我们希望智能体学到的策略能使后面的动作最优，但智能体必须采取非最优的动作才能搜索所有动作。一个更直接的想法是，使用两个策略，目标策略target policy**用于学习成为最优策略**；动作策略behavior policy**用于探索性地生成动作、产生数据**。

#### 覆盖假设

$\pi$和$\mu$需要满足覆盖coverage假设，即当$\pi(s \mid a)>0$时，$\mu(a \mid s)>0$。覆盖假设能保证每个$\pi$下发生的动作能在$\mu$下发生。

#### 重要度采样(importance sampling)

在预测问题中，目标策略$\pi$和和动作策略$\mu$都是已知的，我们要用$\mu$生成采用数据序列，用来评估$\pi$下的状态价值$v_\pi(s)$。在这个过程中，样本数据来自于策略$\mu$产生的分布，实际上需要估计策略$\pi$产生的分布下累计回报的期望。而重要度采样是一种在给定其他样本条件下，估计某个分布下期望值的通用方法，这和我们的需求不谋而合。

下面先介绍重要度采样。

我们需要估计在分布$p(x)$下函数$f(x)$的期望，即

$$
s=\mathbb{E_{p}}[f(\mathbf{x})]=\int_{x} p(\boldsymbol{x}) f(\boldsymbol{x})dx
$$

那我们就需要根据分布$p(x)$抽样n各样本$x^{(1)},…,x^{(n)}$，计算均值得到s的近似：

$$
\hat{s}_{p} = \frac{1}{n}\sum_{i=1,x^{(1)} \sim p }^{n}f(x^{(i)})
$$

这样的经验平均值是无偏的，

$$
\mathbb{E}\left[\hat{s}_{n}\right]=\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}\left[f\left(x^{(i)}\right)\right]=\frac{1}{n} \sum_{i=1}^{n} s=s
$$

并且根据大数定理，样本$x^{(i)}$的独立同分布的，其平均值随着n增大会收敛到期望值。应用在强化学习中，$p(·)$相当于策略$\pi$，$f(·)$相当于累计回报G，而s相当于动作价值$q_{\pi}(s,a)$，以上的推导就是蒙特卡洛算法的理论基础。

在重要度采样中，我们期望不通过分布$p(x)$进行采样，而是通过一定变换，在另一个分布$q(x)$中进行采样，使得采样后计算的经验均值和在$p(x)$中采样得到的一样，即

$$
s = \mathbb{E_{x \sim p}}[f(\mathbf{x})]=\int_x p(x)f(x)d x = \int_x q(x) \frac{p(x)}{q(x)}f(x)d x = \mathbb{E_{x \sim q}}[\frac{p(x)}{q(x)}f(x)]
$$

也就是说，我们根据分布$q(x)$抽样$x^{(i)}$，变换成$\frac{p(x^{(i)})}{q(x^{(i)})}f(x^{(i)})$，它的经验均值为：

$$
\hat{s}_{q} = \frac{1}{n}\sum_{i=1,x^{(i)} \sim q}^{n}\frac{p(x^{(i)})}{q(x^{(i)})}f(x^{(i)})
$$

经验均值$\hat{s}_{q}$也会收敛到到s，这就是重要度采样。

虽然重要度采样的估计$\hat{s}_{q}$的期望不会随着$q(x)$变化(均为s)，但是估计的方差却和$q(x)$有关，根据$Var[X] = \mathbb{E}[X^2]- (\mathbb{E}[X])^2$，

$$
Var_{x \sim p}[f(x)] = \mathbb{E_{x \sim p}}[f(x)^2]- (\mathbb{E_{x\sim p}}[f(x)])^2
$$

$$
\begin{aligned}
Var_{x \sim q}[f(x)\frac{p(x)}{q(x)}]&=\mathbb{E}_{x\sim q}[(f(x)\frac{p(x)}{q(x)})^2]-(\mathbb{E}_{x \sim q}[f(x)\frac{p(x)}{q(x)}])^2 \\
&=\int_x q(x)(f(x)\frac{p(x)}{q(x)})^2d x- (\mathbb{E}_{x \sim q}[f(x)\frac{p(x)}{q(x)}])^2 \\
&=\int_x p(x)(f(x)^2\frac{p(x)}{q(x)}d x- (\mathbb{E}_{x \sim q}[f(x)\frac{p(x)}{q(x)}])^2 \\
&=\mathbb{E}_{x\sim p}[f(x)^2\frac{p(x)}{q(x)}]-(\mathbb{E}_{x\sim p}[f(x)])^2
\end{aligned}
$$

方差就多了一项$\frac{p(x)}{q(x)}$，如果两者分布差距过大，会导致方差变大。

下面验证不同$q(x)$对均值$\mathbb{E}_{x\sim q}[f(x)\frac{p(x)}{q(x)}]$和方差$Var_{x \sim q}[f(x)\frac{p(x)}{q(x)}]$的影响。

```python
from scipy import stats
import numpy as np
# 定义一个函数 f(x)
def f_x(x):
    return 1/(1 + np.exp(-x))

# 定义两个分布 p(x), q(x)
def distribution(mu=0, sigma=1):
    distribution = stats.norm(mu, sigma)
    return distribution

# 采样点数
n = 10

mu_target = 3.5
sigma_target = 1
mu_appro = 3
mu_appro2 = 1
sigma_appro = 1
p_x = distribution(mu_target, sigma_target)
q_x = distribution(mu_appro, sigma_appro)
q_x2 = distribution(mu_appro2, sigma_appro)

list1 = []
list2 = []
list3 = []

for cycle in range(10000):
    ture_list=[]
    for i in range(n):
        x_i = np.random.normal(mu_target, sigma_target)
        ture_list.append(f_x(x_i))
    list1.append(np.mean(ture_list))
    value_list = []
    value_list2 = []
    for i in range(n):
        x_i = np.random.normal(mu_appro, sigma_appro)
        x_i2 = np.random.normal(mu_appro2, sigma_appro)
        value = f_x(x_i)*(p_x.pdf(x_i) / q_x.pdf(x_i))
        value2 = f_x(x_i2)*(p_x.pdf(x_i2) / q_x2.pdf(x_i2))
        value_list.append(value)
        value_list2.append(value2)
    list2.append(np.mean(value_list))
    list3.append(np.mean(value_list2))

print('list1：mean=',np.mean(list1),'variance=',np.var(list1))
print('list2：mean=',np.mean(list2),'variance=',np.var(list2))
print('list3：mean=',np.mean(list3),'variance=',np.var(list3))
```

结果如下，其中按照真实分布采样相当于令$q(x)=p(x)$。重要度采样1使用的分布$q(x)$比重要度采样2更接近原始分布$p(x)$，而重要度采样2的采样序列方差更大。我们可以初步得出结论：$p(x)$和$q(x)$相似程度将影响方差。

|              | $q(x)$              | $\mathbb{E}_{x\sim q}[f(x)\frac{p(x)}{q(x)}]$ | $Var_{x \sim q}[f(x)\frac{p(x)}{q(x)}]$ |
| ------------ | ------------------- | ------ | ------- |
| 真实分布采样 | $x \sim N(3.5,1^2)$ | 0.9550 | 0.0023 |
| 重要度采样1  | $x \sim N(3,1^2)$   | 0.9542 | 0.2967 |
| 重要度采样2  | $x \sim N(1,1^2)$   | 0.9847 | 368.93 |

#### 最优采样函数

虽然重要度采样的估计$\hat{s}_{q}$的期望不会随着$q(x)$变化(均为s)，但是估计的方差却和$q(x)$有关，其方差(这里分子的n和分母的$n^2$抵消了)为：

$$
\operatorname{Var}\left[\hat{s}_{q}\right]=\operatorname{Var}\left[\frac{p(\mathbf{x}) f(\mathbf{x})}{q(\mathbf{x})}\right] / n
$$

为了让方差取到最小，$q(·)$应满足：

$$
q^*(x) = \frac{p(x) \mid f(x) \mid}{Z}
$$

Z为归一化常数。因为$f(x)$可能存在负的情况，而$q(x)$必须大于0，所以需要取绝对值。在$q^*(x)$的条件下，$\operatorname{Var}\left[\hat{s}_{q}\right]=0$，相当于每次采样得到的值都一样，即都是期望s。但是，要估计$q^*(x)$需要计算$p(x)f(x)$，问题难度就和原问题没有区别了。

#### 加权重要度采样

最优采样函数只是方差最小的那个，并不是最适合采样那个。加权重要度采样的$x^{(i)}从$$q(x)$中采样，它是有偏的，即$\mathbb{E}[\hat{s}_{BIS}] \neq s$，但可以降低估计的方差。它的经验均值为：

$$
\begin{aligned}
\hat{s}_{\mathrm{BIS}} &=\frac{\sum_{i=1}^{n} \frac{p\left(x^{(i)}\right)}{q\left(x^{(i)}\right)} f\left(x^{(i)}\right)}{\sum_{i=1}^{n} \frac{p\left(x^{(i)}\right)}{q\left(x^{(i)}\right)}} \\
&= \frac{\sum_{i=1}^{n} \frac{p\left(x^{(i)}\right)}{\tilde{q}\left(x^{(i)}\right)} f\left(x^{(i)}\right)}{\sum_{i=1}^{n} \frac{p\left(x^{(i)}\right)}{\tilde{q}\left(x^{(i)}\right)}} \\
&=\frac{\sum_{i=1}^{n} \frac{\tilde{p}\left(x^{(i)}\right)}{\tilde{q}\left(x^{(i)}\right)} f\left(x^{(i)}\right)}{\sum_{i=1}^{n} \frac{\tilde{p}\left(x^{(i)}\right)}{\tilde{q}\left(x^{(i)}\right)}}
\end{aligned}
$$

$n \to \infty$时且分母收敛到1时，等式才渐近成立，因此也称为渐近无偏，其中$\tilde{q}(x)$和$\tilde{p}(x)$指的是估计值。

下面验证加权重要度采样的性质。加权重要度采样直接给出了经验均值，因此不能求采样点所满足的随机变量的均值和方差。但我们可以求经验均值$\hat{s}_{\mathrm{BIS}}$的期望和方差，我们用$\hat{s}_{q}$的均值和方差作为对比，其中，

$$
\begin{aligned}
Var[\hat{s}_{q}] &=Var[\frac{1}{n}\sum_{i=1,x^{(i)} \sim q}^{n}\frac{p(x^{(i)})}{q(x^{(i)})}f(x^{(i)})]\\
&=\frac{nVar_{x\sim q}[\frac{p(X)}{q(x)}f(x)]}{n^2}\\
&=\frac{Var_{x\sim q}[\frac{p(X)}{q(x)}f(x)]}{n}
\end{aligned}
$$

会随着n的增大而收敛，这里取n=10，在验证重要度采样的程序后面加上：

```python
n = 10
list_wight2 = []
list_wight3 = []

for cycle in range(10000):
    ture_list=[]
    for i in range(n):
        # draw a sample
        x_i = np.random.normal(mu_target, sigma_target)
        ture_list.append(f_x(x_i))
    list1.append(np.mean(ture_list))
    # 从 q(x) 进行重要性采样
    value_list = []
    value_list2 = []
    value_list_weight = []
    value_list_weight2 = []
    for i in range(n):
        # sample from different distribution
        x_i = np.random.normal(mu_appro, sigma_appro)
        x_i2 = np.random.normal(mu_appro2, sigma_appro)
        value = f_x(x_i)*(p_x.pdf(x_i) / q_x.pdf(x_i))
        value2 = f_x(x_i2)*(p_x.pdf(x_i2) / q_x2.pdf(x_i2))
        value_weight = (p_x.pdf(x_i) / q_x.pdf(x_i))
        value_weight2 = (p_x.pdf(x_i2) / q_x2.pdf(x_i2))
        value_list.append(value)
        value_list2.append(value2)
        value_list_weight.append(value_weight)
        value_list_weight2.append(value_weight2)

    list2.append(np.mean(value_list))
    list3.append(np.mean(value_list2))
    list_wight2.append(sum(value_list)/sum(value_list_weight))
    list_wight3.append(sum(value_list2)/sum(value_list_weight2))

print('list1：mean=',np.mean(list1),'variance=',np.var(list1))
print('list2：mean=',np.mean(list2),'variance=',np.var(list2))
print('list_wight2：mean=',np.mean(list_wight2),'variance=',np.var(list_wight2))
print('list3：mean=',np.mean(list3),'variance=',np.var(list3))
print('list_wight3：mean=',np.mean(list_wight3),'variance=',np.var(list_wight3))
```

结果如下：

|                 | q(x)                | $\mathbb{E}[\hat{s}]$ | $Var[\hat{s}]$ |
| --------------- | ------------------- | --------------------- | -------------- |
| 真实分布采样    | $x \sim N(3.5,1^2)$ | 0.9555                | 0.00022        |
| 重要度采样1     | $x \sim N(3,1^2)$   | 0.9587                | 0.03022        |
| 加权重要度采样1 | $x \sim N(3,1^2)$   | 0.9535                | 0.00018        |
| 重要度采样2     | $x \sim N(1,1^2)$   | 0.9564                | 21.9646        |
| 加权重要度采样2 | $x \sim N(1,1^2)$   | 21.964                | 0.00289        |

从结果种可以看出，加权重要度采样确实有降低方差的能力，但是同时估计是有偏的。

#### off-policy+重要度采样

上面介绍了重要度采样的原题，接下来将它应用于off-policy，我们根据**某一采样序列在目标策略和动作策略下出现的相对概率**对回报值进行加权，定义这个相对概率为**重要度采样比**。在初始状态$S_t$下，后续出现采样序列$A_t,S_{s+1},A_{t+1},…,S_T$在策略$\pi$下出现的概率为：

$$
Pr\{A_t,S_{s+1},A_{t+1},…,S_t \mid S_t,A_{t:T-1}\sim\pi\} =\prod_{k=t}^{T-1} \pi\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)
$$

重要度采样比为：

$$
\rho_{t:T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)}{\prod_{k=t}^{T-1} \mu\left(A_{k} \mid S_{k}\right) p\left(S_{k+1} \mid S_{k}, A_{k}\right)}=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} \mid S_{k}\right)}{\mu\left(A_{k} \mid S_{k}\right)}
$$

这个值和状态转移概率无关，因此说和MDP的动态特性无关。状态价值$v_\pi(s)$是从状态s开始的累计回报G的期望，即：

$$
v_\pi(s) = \mathbb{E}_{\pi}[G_t \mid S_t = s]=\mathbb{E}_{\mu}[\rho_{t:T-1}G_t \mid S_t = s]
$$

接下来说明一下记号：

- 假设状态价值$v_{\pi}(s)$的预测值$V(s)$；
- 为了记号方便，我们对时刻t进行编号，跨越episode的边界序号也递增；
- 将访问过状态s的时刻集合记作$\mathcal{T}(s)$，即$\mathcal{T}(s)=\{t\mid S_t = s\}$；
- 对于某个时刻t，$T(t)$表示t之后的首次终止状态。

那么$G_t$表示的就是从t到$T(t)$的累计回报，$\{G_t\}_{t \in \mathcal{T(s)}}$表示的是从状态s开始的累计回报集合，$\{\rho_{t:T(t)}\}_{t\in\mathcal{T}(s)}$表示相应的重要度采样比。用重要度采样对$v_{\pi}(s)$进行估计：

$$
V(s) \doteq \frac{\sum_{t \in \mathrm{T}(s)} \rho_{t:T(t)-1} G_{t}}{|\mathcal{T}(s)|}
$$

相应的加权重要度采样为：

$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_{t}}{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1}}
$$

和前文提到的性质相同，普通重要度采样是无偏估计，但方差大；加权重要度采样是有偏估计，但是方差更小。

---

## 4 增量式实现

增量式实现只是在蒙特卡洛算法具体实现时的一个技巧，每次获得新的采样累计回报$G_n$后，不需要重新计算均值，只需要上一步的均值$V_{n-1}$和$G_n$按照权重系数进行线性组合即可。

假设获得了累计回报序列$G_1,G_2,…,G_{n-1}$，相应的权重为$W_i = \rho_{t:T(t)-1}$。在第(n-1)步时，我们已经计算了

$$
V_{n-1} \doteq \frac{\sum_{k=1}^{n-1} W_{k} G_{k}}{\sum_{k=1}^{n-1} W_{k}}
$$

在n步，我们获得了累计回报$G_n$和相应的权重$W_k$，那么$n \geq1 $时，

$$
\begin{aligned}
V_{n} &\doteq \frac{\sum_{k=1}^{n} W_{k} G_{k}}{\sum_{k=1}^{n} W_{k}}\\
&=\frac{\sum_{k=1}^{n-1} W_{k} G_{k} + W_nG_n}{\sum_{k=1}^{n-1} W_{k}+W_n}\\
&=\frac{\sum_{k=1}^{n-1} W_{k} V_{n-1} + W_nG_n}{\sum_{k=1}^{n-1} W_{k}+W_n}\\
&=\frac{\sum_{k=1}^{n-1} W_{k} V_{n-1} +W_n V_{n-1}+ W_nG_n-W_n V_{n-1}}{\sum_{k=1}^{n-1} W_{k}+W_n}\\
&=V_{n-1}+\frac{W_n}{\sum_{k=1}^{n} W_{k}}(G_n-V_{n-1})
\end{aligned}
$$

记$C_n = {\sum_{k=1}^{n} W_{k}} = C_{n-1}+W_n$，每次更新时，

$$
C_n = C_{n-1}+W_n
$$

$$
V_{n} = V_{n-1}+\frac{W_n}{C_n}(G_n-V_{n-1})
$$

初值为$V_0=0$，$C_0=0$。

下面是增量式计算的算法框图：

![image-20210401152151127](https://github.com/tlt18/tlt18.github.io/raw/master/fig/2021-03-28/image-20210401152151127.png)

---

## 5 off-policy蒙特卡洛控制 

下面是off-policy每次访问的蒙特卡洛控制的算法框图：

![image-20210401164027351](https://github.com/tlt18/tlt18.github.io/raw/master/fig/2021-03-28/image-20210401164027351.png)

这个蒙特卡洛控制方法基于GPI和重要度采样，选择的behavior policy是$\varepsilon-soft$的。这里遗留个**问题**：实际需要用重要度采样比$\frac{\pi(A_t \mid S_t)}{\mu(A_t \mid S_t)}$更新W(这里是从一个episode的尾部开始累乘的)，但这里只用了$\frac{1}{\mu(A_t \mid S_t)}$，为什么是正确的？