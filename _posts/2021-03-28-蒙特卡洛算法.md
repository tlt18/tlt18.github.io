---
title: [强化学习]蒙特卡洛方法
tags: RL
key: 2021-03-29
author: tlt18
modify_date: 2021-03-28
---

蒙特卡洛方法(Monte Carlo method)是通过采样，用某个随机事件出现的频率估计该随机事件的概率、随机变量的期望的方法。本文将介绍在强化学习中利用蒙特卡洛方法估计行动价值$q_\pi(s,a)$(预测问题)、更新策略(控制问题)。

<!--more-->

## 预测问题 Monte Carlo Prediction

### 基本方法

预测问题的目标是，在给定一个策略$\pi$后，计算行动价值函数$q_\pi(s,a)$。根据定义，行动价值函数是在给定的状态s、行动a下，累计回报G的期望：



$$
q_\pi(s,a)=\mathbb{E_\pi}[G_t \mid S_t=s,A_t=a]
$$


其中累计回报是从当前时刻开始，折现回报之和：


$$
G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
$$


使用蒙特卡洛方法，是将行动价值计算公式中的数学期望，替换成采样累计回报G的平均值：


$$
q_{\pi}(s,a) \approx \frac{1}{n} \sum_{i=1}^{n} G_{i}
$$


其中采样时采用策略$\pi$，生成一系列观测片段，就能保证是在$\pi$产生的分布下计算的期望。例如我们得到一个观测片段$
S_{1}, A_{1}, R_{2}, \ldots, S_{t}=s, A_{t}=a, R_{t+1}, \ldots, S_{T}, A_{T}, R_{T+1}
$,得到累计回报G的一个采样：


$$
G_{i}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-t} R_{T+1}
$$


多次采样后，得到针对(s,a)的一系列累计回报$G_1,G_2,\cdots,G_n$，取均值就得到$q_{\pi}(s,a)$的一个估计。

### 首次访问和每次访问